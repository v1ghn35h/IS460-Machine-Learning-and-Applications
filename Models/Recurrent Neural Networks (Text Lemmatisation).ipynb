{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6dd8cb2d987f4bbd8283cfdfde61d901",
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": []
   },
   "source": [
    "# Recurrent Neural Networks (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "161a4dcf3ea9420b9ff68d6473491644",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 9845,
    "execution_start": 1699006102539,
    "source_hash": "d04263c1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statistics\n",
    "import plotly.express as px\n",
    "\n",
    "# Train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Word2Vec Embedding\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "from gensim.models import FastText\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "\n",
    "from keras.utils import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# ELMo Embedding\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import h5py\n",
    "\n",
    "# Hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "#RNN\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from scikeras.wrappers import KerasClassifier \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, Dense, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b3915e26bb554eefa71dbe4dcc14fa88",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": [],
    "is_collapsed": false
   },
   "source": [
    "## Load cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "c55cc65ddc9344c5ad42620f917532d4",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 558,
    "execution_start": 1699006114365,
    "source_hash": "d1ce2be3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categorization</th>\n",
       "      <th>Body</th>\n",
       "      <th>Label</th>\n",
       "      <th>Cleaned Text</th>\n",
       "      <th>Cleaned Text with N lemmatization</th>\n",
       "      <th>Cleaned Text with V lemmatization</th>\n",
       "      <th>Cleaned Text with A lemmatization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Envy to other is swallowing me</td>\n",
       "      <td>Im from developingcountry, Indonesia , and for...</td>\n",
       "      <td>1</td>\n",
       "      <td>im developingcountry indonesia temporary work ...</td>\n",
       "      <td>im developingcountry indonesia temporary work ...</td>\n",
       "      <td>im developingcountry indonesia temporary work ...</td>\n",
       "      <td>im developingcountry indonesia temporary work ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nothin outta the ordinary. Paradise. Job stres...</td>\n",
       "      <td>Um hello ....well many can relate im sure. Aft...</td>\n",
       "      <td>1</td>\n",
       "      <td>um hello well many relate im sure today im con...</td>\n",
       "      <td>um hello well many relate im sure today im con...</td>\n",
       "      <td>um hello well many relate im sure today im con...</td>\n",
       "      <td>um hello well many relate im sure today im con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Almost 49 and the chasm of emptiness has never...</td>\n",
       "      <td>I’ve been diagnosed severe bi polar where you ...</td>\n",
       "      <td>1</td>\n",
       "      <td>ive diagnosed severe bi polar longer even get ...</td>\n",
       "      <td>ive diagnosed severe bi polar longer even get ...</td>\n",
       "      <td>ive diagnose severe bi polar longer even get g...</td>\n",
       "      <td>ive diagnosed severe bi polar long even get go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I’m happy again</td>\n",
       "      <td>After my closest friend left me in April, I ha...</td>\n",
       "      <td>0</td>\n",
       "      <td>closest friend left april finally let go reali...</td>\n",
       "      <td>closest friend left april finally let go reali...</td>\n",
       "      <td>closest friend leave april finally let go real...</td>\n",
       "      <td>close friend left april finally let go realize...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Is it possible to recover from such a traumati...</td>\n",
       "      <td>I am only 15, and yet I feel my life is alread...</td>\n",
       "      <td>1</td>\n",
       "      <td>15 yet feel life already pit emptiness stomach...</td>\n",
       "      <td>15 yet feel life already pit emptiness stomach...</td>\n",
       "      <td>15 yet feel life already pit emptiness stomach...</td>\n",
       "      <td>15 yet feel life already pit emptiness stomach...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Categorization  \\\n",
       "0                     Envy to other is swallowing me   \n",
       "1  Nothin outta the ordinary. Paradise. Job stres...   \n",
       "2  Almost 49 and the chasm of emptiness has never...   \n",
       "3                                    I’m happy again   \n",
       "4  Is it possible to recover from such a traumati...   \n",
       "\n",
       "                                                Body  Label  \\\n",
       "0  Im from developingcountry, Indonesia , and for...      1   \n",
       "1  Um hello ....well many can relate im sure. Aft...      1   \n",
       "2  I’ve been diagnosed severe bi polar where you ...      1   \n",
       "3  After my closest friend left me in April, I ha...      0   \n",
       "4  I am only 15, and yet I feel my life is alread...      1   \n",
       "\n",
       "                                        Cleaned Text  \\\n",
       "0  im developingcountry indonesia temporary work ...   \n",
       "1  um hello well many relate im sure today im con...   \n",
       "2  ive diagnosed severe bi polar longer even get ...   \n",
       "3  closest friend left april finally let go reali...   \n",
       "4  15 yet feel life already pit emptiness stomach...   \n",
       "\n",
       "                   Cleaned Text with N lemmatization  \\\n",
       "0  im developingcountry indonesia temporary work ...   \n",
       "1  um hello well many relate im sure today im con...   \n",
       "2  ive diagnosed severe bi polar longer even get ...   \n",
       "3  closest friend left april finally let go reali...   \n",
       "4  15 yet feel life already pit emptiness stomach...   \n",
       "\n",
       "                   Cleaned Text with V lemmatization  \\\n",
       "0  im developingcountry indonesia temporary work ...   \n",
       "1  um hello well many relate im sure today im con...   \n",
       "2  ive diagnose severe bi polar longer even get g...   \n",
       "3  closest friend leave april finally let go real...   \n",
       "4  15 yet feel life already pit emptiness stomach...   \n",
       "\n",
       "                   Cleaned Text with A lemmatization  \n",
       "0  im developingcountry indonesia temporary work ...  \n",
       "1  um hello well many relate im sure today im con...  \n",
       "2  ive diagnosed severe bi polar long even get go...  \n",
       "3  close friend left april finally let go realize...  \n",
       "4  15 yet feel life already pit emptiness stomach...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read from CSV file\n",
    "df = pd.read_csv('cleaned_combined_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "877c730e676b4998b69df59949875acb",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 15,
    "execution_start": 1699006117826,
    "source_hash": "1bd18b6b"
   },
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "83ffcc85578c42a38f1861aa1ebe2598",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": []
   },
   "source": [
    "## Hypothesis 1: How does different text lemmatization affect the model results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, make_scorer\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "3ecf9a7115984c0da9721a803939f5cf",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 10,
    "execution_start": 1699007259546,
    "source_hash": "50138590"
   },
   "outputs": [],
   "source": [
    "def create_rnn_model(embedding_dim, num_units, learning_rate, vocab_size, max_sequence_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n",
    "    model.add(SimpleRNN(units=num_units, activation='tanh'))\n",
    "    model.add(Dense(64, activation='tanh'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "    return model\n",
    "\n",
    "\n",
    "def hypothesis1_test(column_name, perform_hyperparameter_tuning=False):\n",
    "    # Initialize f1_score and recall\n",
    "    f1 = 0.0\n",
    "    recall = 0.0\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X = df[column_name]\n",
    "    y = df['Label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "    # Define the vocabulary size\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "    # Set the maximum sequence length (adjust as needed)\n",
    "    max_sequence_length = 100  # You can choose an appropriate sequence length\n",
    "\n",
    "    if perform_hyperparameter_tuning:\n",
    "        # Define the hyperparameter distributions for RandomizedSearchCV\n",
    "        param_dist = {\n",
    "            'embedding_dim': np.arange(50, 151, 10),  # Range of values for embedding_dim\n",
    "            'num_units': np.arange(32, 193, 32),      # Range of values for num_units\n",
    "            'learning_rate': [1e-2, 1e-3, 1e-4]\n",
    "        }\n",
    "\n",
    "        # Create a custom scorer for F1-score using the f1_score function\n",
    "        recall_scorer = make_scorer(recall_score)\n",
    "\n",
    "        # Create the RandomizedSearchCV\n",
    "        rnn_model = KerasClassifier(model=create_rnn_model, vocab_size=vocab_size, max_sequence_length=max_sequence_length, num_units=32, learning_rate=0.01, embedding_dim=50)\n",
    "        random_search = RandomizedSearchCV(estimator=rnn_model, param_distributions=param_dist, scoring=recall_scorer, cv=5, n_iter=10, verbose=1)\n",
    "\n",
    "        # Convert text to sequences and pad them to the specified length\n",
    "        X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=max_sequence_length)\n",
    "\n",
    "        # Fit the random search to the training data with numerical features\n",
    "        random_search.fit(X_train_seq, y_train)\n",
    "\n",
    "        best_hyperparameters = random_search.best_params_\n",
    "        # Create the best model with the best hyperparameters\n",
    "        best_model = create_rnn_model(embedding_dim=best_hyperparameters['embedding_dim'],\n",
    "                                      num_units=best_hyperparameters['num_units'],\n",
    "                                      learning_rate=best_hyperparameters['learning_rate'],\n",
    "                                      vocab_size=vocab_size,\n",
    "                                      max_sequence_length=max_sequence_length)\n",
    "\n",
    "        # Compile the best model\n",
    "        best_model.compile(optimizer=keras.optimizers.Adam(learning_rate=best_hyperparameters['learning_rate']),\n",
    "                          loss='binary_crossentropy',\n",
    "                          metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "\n",
    "        # Use the best model to make predictions on the test set\n",
    "        X_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=max_sequence_length)\n",
    "        y_test_pred = (best_model.predict(X_test_seq) > 0.5).astype(int)\n",
    "\n",
    "        # Calculate classification error\n",
    "        accuracy = accuracy_score(y_test, y_test_pred)\n",
    "        classification_error = 1 - accuracy\n",
    "\n",
    "        # Calculate recall, specificity, F1 score, and AUC score\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_test_pred).ravel()\n",
    "        recall = tp / (tp + fn)\n",
    "        specificity = tn / (tn + fp)\n",
    "        precision = precision_score(y_test, y_test_pred)\n",
    "        f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "        auc_score = roc_auc_score(y_test, y_test_pred)\n",
    "\n",
    "        print(f\"Test accuracy: {accuracy * 100:.2f}%\")\n",
    "        print(f\"Classification Error: {classification_error * 100:.2f}%\")\n",
    "        print(f\"Recall: {recall * 100:.2f}%\")\n",
    "        print(f\"Specificity: {specificity * 100:.2f}%\")\n",
    "        print(f\"Precision: {precision * 100:.2f}%\")\n",
    "        print(f\"F1 Score: {f1 * 100:.2f}%\")\n",
    "        print(f\"AUC Score: {auc_score * 100:.2f}%\")\n",
    "\n",
    "        print(\"Best Hyperparameters:\")\n",
    "        print(f\"Embedding Dim: {best_hyperparameters.get('embedding_dim')}\")\n",
    "        print(f\"Num Units: {best_hyperparameters.get('num_units')}\")\n",
    "        print(f\"Learning Rate: {best_hyperparameters.get('learning_rate')}\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Convert text to sequences and pad them to the specified length\n",
    "        X_train = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=max_sequence_length)\n",
    "        X_test = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=max_sequence_length)\n",
    "\n",
    "        embedding_dim = 100  # Set your desired embedding dimension\n",
    "\n",
    "        model = create_rnn_model(embedding_dim=embedding_dim, num_units=32, learning_rate=0.01, vocab_size=vocab_size, max_sequence_length=max_sequence_length)\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss='binary_crossentropy',\n",
    "                      metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "\n",
    "        batch_size = 32  # Set your desired batch size\n",
    "        num_epochs = 10  # Set the number of training epochs\n",
    "\n",
    "        model.fit(X_train, y_train, batch_size=batch_size, epochs=num_epochs)\n",
    "\n",
    "        result = model.evaluate(X_test, y_test)\n",
    "        loss = result[0]\n",
    "        accuracy = result[1]\n",
    "\n",
    "        # Use the model to make predictions on the test set\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred = (y_pred > 0.5).astype(int)  # Convert predicted probabilities to binary labels\n",
    "\n",
    "        # Calculate classification error\n",
    "        classification_error = 1 - accuracy\n",
    "\n",
    "        # Calculate recall, specificity, F1 score, and AUC score\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        recall = tp / (tp + fn)\n",
    "        specificity = tn / (tn + fp)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        auc_score = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "        print(f\"Test accuracy: {accuracy * 100:.2f}%\")\n",
    "        print(f\"Classification Error: {classification_error * 100:.2f}%\")\n",
    "        print(f\"Recall: {recall * 100:.2f}%\")\n",
    "        print(f\"Specificity: {specificity * 100:.2f}%\")\n",
    "        print(f\"Precision: {precision * 100:.2f}%\")\n",
    "        print(f\"F1 Score: {f1 * 100:.2f}%\")\n",
    "        print(f\"AUC Score: {auc_score * 100:.2f}%\")\n",
    "\n",
    "        print(classification_report(y_test, y_pred))  # This\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "f2fed8c4b6fd4730ae11e9bd1923923b",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5260,
    "execution_start": 1699007260946,
    "source_hash": "f46c0ca4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "360/360 [==============================] - 9s 23ms/step - loss: 0.5548 - accuracy: 0.7280 - precision_1: 0.7426 - recall_1: 0.8340\n",
      "Epoch 2/10\n",
      "360/360 [==============================] - 8s 23ms/step - loss: 0.3793 - accuracy: 0.8424 - precision_1: 0.8571 - recall_1: 0.8835\n",
      "Epoch 3/10\n",
      "360/360 [==============================] - 8s 22ms/step - loss: 0.2927 - accuracy: 0.8810 - precision_1: 0.8880 - recall_1: 0.9165\n",
      "Epoch 4/10\n",
      "360/360 [==============================] - 8s 22ms/step - loss: 0.2381 - accuracy: 0.9084 - precision_1: 0.9175 - recall_1: 0.9302\n",
      "Epoch 5/10\n",
      "360/360 [==============================] - 8s 22ms/step - loss: 0.2098 - accuracy: 0.9231 - precision_1: 0.9311 - recall_1: 0.9409\n",
      "Epoch 6/10\n",
      "360/360 [==============================] - 8s 22ms/step - loss: 0.1847 - accuracy: 0.9343 - precision_1: 0.9407 - recall_1: 0.9499\n",
      "Epoch 7/10\n",
      "360/360 [==============================] - 8s 22ms/step - loss: 0.1627 - accuracy: 0.9415 - precision_1: 0.9450 - recall_1: 0.9579\n",
      "Epoch 8/10\n",
      "360/360 [==============================] - 8s 22ms/step - loss: 0.1737 - accuracy: 0.9358 - precision_1: 0.9407 - recall_1: 0.9525\n",
      "Epoch 9/10\n",
      "360/360 [==============================] - 8s 23ms/step - loss: 0.1655 - accuracy: 0.9398 - precision_1: 0.9433 - recall_1: 0.9567\n",
      "Epoch 10/10\n",
      "360/360 [==============================] - 8s 23ms/step - loss: 0.1837 - accuracy: 0.9286 - precision_1: 0.9333 - recall_1: 0.9483\n",
      "90/90 [==============================] - 0s 3ms/step - loss: 0.8904 - accuracy: 0.7062 - precision_1: 0.7418 - recall_1: 0.7535\n",
      "90/90 [==============================] - 0s 3ms/step\n",
      "Test accuracy: 70.62%\n",
      "Classification Error: 29.38%\n",
      "Recall: 75.35%\n",
      "Specificity: 64.17%\n",
      "Precision: 74.18%\n",
      "F1 Score: 74.76%\n",
      "AUC Score: 69.76%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.64      0.65      1214\n",
      "           1       0.74      0.75      0.75      1659\n",
      "\n",
      "    accuracy                           0.71      2873\n",
      "   macro avg       0.70      0.70      0.70      2873\n",
      "weighted avg       0.71      0.71      0.71      2873\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hypothesis1_test('Cleaned Text', perform_hyperparameter_tuning=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "f2fed8c4b6fd4730ae11e9bd1923923b",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5260,
    "execution_start": 1699007260946,
    "source_hash": "f46c0ca4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "360/360 [==============================] - 9s 24ms/step - loss: 0.5532 - accuracy: 0.7276 - precision_3: 0.7377 - recall_3: 0.8445\n",
      "Epoch 2/10\n",
      "360/360 [==============================] - 9s 24ms/step - loss: 0.3817 - accuracy: 0.8395 - precision_3: 0.8530 - recall_3: 0.8838\n",
      "Epoch 3/10\n",
      "360/360 [==============================] - 9s 24ms/step - loss: 0.3243 - accuracy: 0.8717 - precision_3: 0.8793 - recall_3: 0.9103\n",
      "Epoch 4/10\n",
      "360/360 [==============================] - 9s 24ms/step - loss: 0.3096 - accuracy: 0.8773 - precision_3: 0.8944 - recall_3: 0.9010\n",
      "Epoch 5/10\n",
      "360/360 [==============================] - 9s 24ms/step - loss: 0.2638 - accuracy: 0.8948 - precision_3: 0.9103 - recall_3: 0.9139\n",
      "Epoch 6/10\n",
      "360/360 [==============================] - 9s 24ms/step - loss: 0.2194 - accuracy: 0.9151 - precision_3: 0.9250 - recall_3: 0.9336\n",
      "Epoch 7/10\n",
      "360/360 [==============================] - 9s 24ms/step - loss: 0.1812 - accuracy: 0.9314 - precision_3: 0.9421 - recall_3: 0.9432\n",
      "Epoch 8/10\n",
      "360/360 [==============================] - 9s 24ms/step - loss: 0.1913 - accuracy: 0.9276 - precision_3: 0.9355 - recall_3: 0.9439\n",
      "Epoch 9/10\n",
      "360/360 [==============================] - 9s 24ms/step - loss: 0.1825 - accuracy: 0.9310 - precision_3: 0.9410 - recall_3: 0.9436\n",
      "Epoch 10/10\n",
      "360/360 [==============================] - 9s 24ms/step - loss: 0.1677 - accuracy: 0.9386 - precision_3: 0.9469 - recall_3: 0.9505\n",
      "90/90 [==============================] - 0s 3ms/step - loss: 0.8039 - accuracy: 0.7275 - precision_3: 0.7570 - recall_3: 0.7776\n",
      "90/90 [==============================] - 0s 3ms/step\n",
      "Test accuracy: 72.75%\n",
      "Classification Error: 27.25%\n",
      "Recall: 77.76%\n",
      "Specificity: 65.90%\n",
      "Precision: 75.70%\n",
      "F1 Score: 76.72%\n",
      "AUC Score: 71.83%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.66      0.67      1214\n",
      "           1       0.76      0.78      0.77      1659\n",
      "\n",
      "    accuracy                           0.73      2873\n",
      "   macro avg       0.72      0.72      0.72      2873\n",
      "weighted avg       0.73      0.73      0.73      2873\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hypothesis1_test('Cleaned Text with N lemmatization', perform_hyperparameter_tuning=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "f2fed8c4b6fd4730ae11e9bd1923923b",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5260,
    "execution_start": 1699007260946,
    "source_hash": "f46c0ca4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "360/360 [==============================] - 9s 22ms/step - loss: 0.5538 - accuracy: 0.7325 - precision_5: 0.7355 - recall_5: 0.8624\n",
      "Epoch 2/10\n",
      "360/360 [==============================] - 8s 22ms/step - loss: 0.4344 - accuracy: 0.8107 - precision_5: 0.8173 - recall_5: 0.8798\n",
      "Epoch 3/10\n",
      "360/360 [==============================] - 8s 24ms/step - loss: 0.3684 - accuracy: 0.8435 - precision_5: 0.8517 - recall_5: 0.8938\n",
      "Epoch 4/10\n",
      "360/360 [==============================] - 8s 23ms/step - loss: 0.3273 - accuracy: 0.8698 - precision_5: 0.8785 - recall_5: 0.9077\n",
      "Epoch 5/10\n",
      "360/360 [==============================] - 7s 20ms/step - loss: 0.2967 - accuracy: 0.8842 - precision_5: 0.8903 - recall_5: 0.9195\n",
      "Epoch 6/10\n",
      "360/360 [==============================] - 7s 19ms/step - loss: 0.2774 - accuracy: 0.8896 - precision_5: 0.8992 - recall_5: 0.9181\n",
      "Epoch 7/10\n",
      "360/360 [==============================] - 7s 19ms/step - loss: 0.2416 - accuracy: 0.9082 - precision_5: 0.9130 - recall_5: 0.9355\n",
      "Epoch 8/10\n",
      "360/360 [==============================] - 7s 20ms/step - loss: 0.2300 - accuracy: 0.9100 - precision_5: 0.9149 - recall_5: 0.9365\n",
      "Epoch 9/10\n",
      "360/360 [==============================] - 7s 19ms/step - loss: 0.2025 - accuracy: 0.9231 - precision_5: 0.9326 - recall_5: 0.9391\n",
      "Epoch 10/10\n",
      "360/360 [==============================] - 7s 20ms/step - loss: 0.1973 - accuracy: 0.9251 - precision_5: 0.9312 - recall_5: 0.9444\n",
      "90/90 [==============================] - 0s 3ms/step - loss: 0.8610 - accuracy: 0.7198 - precision_5: 0.7412 - recall_5: 0.7908\n",
      "90/90 [==============================] - 0s 3ms/step\n",
      "Test accuracy: 71.98%\n",
      "Classification Error: 28.02%\n",
      "Recall: 79.08%\n",
      "Specificity: 62.27%\n",
      "Precision: 74.12%\n",
      "F1 Score: 76.52%\n",
      "AUC Score: 70.68%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.62      0.65      1214\n",
      "           1       0.74      0.79      0.77      1659\n",
      "\n",
      "    accuracy                           0.72      2873\n",
      "   macro avg       0.71      0.71      0.71      2873\n",
      "weighted avg       0.72      0.72      0.72      2873\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hypothesis1_test('Cleaned Text with V lemmatization', perform_hyperparameter_tuning=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "f2fed8c4b6fd4730ae11e9bd1923923b",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5260,
    "execution_start": 1699007260946,
    "source_hash": "f46c0ca4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "360/360 [==============================] - 9s 22ms/step - loss: 0.5397 - accuracy: 0.7365 - precision_7: 0.7557 - recall_7: 0.8261\n",
      "Epoch 2/10\n",
      "360/360 [==============================] - 8s 22ms/step - loss: 0.3577 - accuracy: 0.8559 - precision_7: 0.8665 - recall_7: 0.8970\n",
      "Epoch 3/10\n",
      "360/360 [==============================] - 8s 22ms/step - loss: 0.2819 - accuracy: 0.8915 - precision_7: 0.8998 - recall_7: 0.9209\n",
      "Epoch 4/10\n",
      "360/360 [==============================] - 8s 22ms/step - loss: 0.2161 - accuracy: 0.9159 - precision_7: 0.9231 - recall_7: 0.9374\n",
      "Epoch 5/10\n",
      "360/360 [==============================] - 8s 22ms/step - loss: 0.1759 - accuracy: 0.9358 - precision_7: 0.9410 - recall_7: 0.9522\n",
      "Epoch 6/10\n",
      "360/360 [==============================] - 8s 22ms/step - loss: 0.1477 - accuracy: 0.9467 - precision_7: 0.9506 - recall_7: 0.9607\n",
      "Epoch 7/10\n",
      "360/360 [==============================] - 8s 22ms/step - loss: 0.1457 - accuracy: 0.9451 - precision_7: 0.9524 - recall_7: 0.9559\n",
      "Epoch 8/10\n",
      "360/360 [==============================] - 8s 22ms/step - loss: 0.1584 - accuracy: 0.9394 - precision_7: 0.9476 - recall_7: 0.9512\n",
      "Epoch 9/10\n",
      "360/360 [==============================] - 8s 22ms/step - loss: 0.1371 - accuracy: 0.9517 - precision_7: 0.9583 - recall_7: 0.9610\n",
      "Epoch 10/10\n",
      "360/360 [==============================] - 8s 22ms/step - loss: 0.1511 - accuracy: 0.9447 - precision_7: 0.9534 - recall_7: 0.9540\n",
      "90/90 [==============================] - 0s 3ms/step - loss: 0.8231 - accuracy: 0.7139 - precision_7: 0.7321 - recall_7: 0.7957\n",
      "90/90 [==============================] - 0s 3ms/step\n",
      "Test accuracy: 71.39%\n",
      "Classification Error: 28.61%\n",
      "Recall: 79.57%\n",
      "Specificity: 60.21%\n",
      "Precision: 73.21%\n",
      "F1 Score: 76.26%\n",
      "AUC Score: 69.89%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.60      0.64      1214\n",
      "           1       0.73      0.80      0.76      1659\n",
      "\n",
      "    accuracy                           0.71      2873\n",
      "   macro avg       0.71      0.70      0.70      2873\n",
      "weighted avg       0.71      0.71      0.71      2873\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hypothesis1_test('Cleaned Text with A lemmatization', perform_hyperparameter_tuning=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "8a1e62bd6f544ddf9172391c0680c25e",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "288/288 [==============================] - 12s 39ms/step - loss: 0.6428 - accuracy: 0.6261 - precision_8: 0.6303 - recall_8: 0.9051\n",
      "72/72 [==============================] - 0s 5ms/step\n",
      "288/288 [==============================] - 12s 38ms/step - loss: 0.6579 - accuracy: 0.6124 - precision_9: 0.6163 - recall_9: 0.9306\n",
      "72/72 [==============================] - 0s 4ms/step\n",
      "288/288 [==============================] - 12s 38ms/step - loss: 0.6532 - accuracy: 0.6132 - precision_10: 0.6230 - recall_10: 0.8928\n",
      "72/72 [==============================] - 0s 5ms/step\n",
      "288/288 [==============================] - 12s 38ms/step - loss: 0.6483 - accuracy: 0.6258 - precision_11: 0.6300 - recall_11: 0.9055\n",
      "72/72 [==============================] - 0s 5ms/step\n",
      "288/288 [==============================] - 12s 39ms/step - loss: 0.6365 - accuracy: 0.6339 - precision_12: 0.6453 - recall_12: 0.8598\n",
      "72/72 [==============================] - 0s 5ms/step\n",
      "288/288 [==============================] - 10s 30ms/step - loss: 0.6575 - accuracy: 0.6091 - precision_13: 0.6162 - recall_13: 0.9168\n",
      "72/72 [==============================] - 0s 4ms/step\n",
      "288/288 [==============================] - 10s 30ms/step - loss: 0.6361 - accuracy: 0.6409 - precision_14: 0.6543 - recall_14: 0.8462\n",
      "72/72 [==============================] - 0s 3ms/step\n",
      "288/288 [==============================] - 8s 26ms/step - loss: 0.6545 - accuracy: 0.6167 - precision_15: 0.6149 - recall_15: 0.9590\n",
      "72/72 [==============================] - 0s 3ms/step\n",
      "288/288 [==============================] - 8s 26ms/step - loss: 0.6413 - accuracy: 0.6352 - precision_16: 0.6377 - recall_16: 0.9019\n",
      "72/72 [==============================] - 0s 3ms/step\n",
      "288/288 [==============================] - 8s 26ms/step - loss: 0.6587 - accuracy: 0.6065 - precision_17: 0.6155 - recall_17: 0.9097\n",
      "72/72 [==============================] - 0s 3ms/step\n",
      "288/288 [==============================] - 11s 34ms/step - loss: 0.6597 - accuracy: 0.6091 - precision_18: 0.6146 - recall_18: 0.9273\n",
      "72/72 [==============================] - 0s 6ms/step\n",
      "288/288 [==============================] - 11s 34ms/step - loss: 0.6708 - accuracy: 0.5945 - precision_19: 0.6037 - recall_19: 0.9356\n",
      "72/72 [==============================] - 0s 6ms/step\n",
      "288/288 [==============================] - 14s 46ms/step - loss: 0.6518 - accuracy: 0.6167 - precision_20: 0.6255 - recall_20: 0.8933\n",
      "72/72 [==============================] - 1s 8ms/step\n",
      "288/288 [==============================] - 12s 40ms/step - loss: 0.6577 - accuracy: 0.6168 - precision_21: 0.6190 - recall_21: 0.9330\n",
      "72/72 [==============================] - 1s 7ms/step\n",
      "288/288 [==============================] - 11s 36ms/step - loss: 0.6596 - accuracy: 0.6003 - precision_22: 0.6106 - recall_22: 0.9143\n",
      "72/72 [==============================] - 1s 6ms/step\n",
      "288/288 [==============================] - 8s 26ms/step - loss: 0.6459 - accuracy: 0.6366 - precision_23: 0.6408 - recall_23: 0.8915\n",
      "72/72 [==============================] - 0s 3ms/step\n",
      "288/288 [==============================] - 8s 26ms/step - loss: 0.6077 - accuracy: 0.6718 - precision_24: 0.6708 - recall_24: 0.8853\n",
      "72/72 [==============================] - 0s 3ms/step\n",
      "288/288 [==============================] - 8s 26ms/step - loss: 0.6467 - accuracy: 0.6320 - precision_25: 0.6394 - recall_25: 0.8809\n",
      "72/72 [==============================] - 0s 3ms/step\n",
      "288/288 [==============================] - 8s 25ms/step - loss: 0.6734 - accuracy: 0.5877 - precision_26: 0.5969 - recall_26: 0.9545\n",
      "72/72 [==============================] - 0s 3ms/step\n",
      "288/288 [==============================] - 9s 26ms/step - loss: 0.6412 - accuracy: 0.6395 - precision_27: 0.6519 - recall_27: 0.8511\n",
      "72/72 [==============================] - 0s 3ms/step\n",
      "288/288 [==============================] - 6s 18ms/step - loss: 0.6613 - accuracy: 0.6284 - precision_28: 0.6551 - recall_28: 0.7984\n",
      "72/72 [==============================] - 0s 3ms/step\n",
      "288/288 [==============================] - 6s 18ms/step - loss: 0.6261 - accuracy: 0.6649 - precision_29: 0.6748 - recall_29: 0.8476\n",
      "72/72 [==============================] - 0s 3ms/step\n",
      "288/288 [==============================] - 6s 17ms/step - loss: 0.6466 - accuracy: 0.6335 - precision_30: 0.6582 - recall_30: 0.8045\n",
      "72/72 [==============================] - 0s 2ms/step\n",
      "288/288 [==============================] - 6s 17ms/step - loss: 0.6682 - accuracy: 0.6106 - precision_31: 0.6485 - recall_31: 0.7604\n",
      "72/72 [==============================] - 0s 2ms/step\n",
      "288/288 [==============================] - 6s 18ms/step - loss: 0.6248 - accuracy: 0.6540 - precision_32: 0.6794 - recall_32: 0.7968\n",
      "72/72 [==============================] - 0s 2ms/step\n",
      "288/288 [==============================] - 5s 13ms/step - loss: 0.6511 - accuracy: 0.6259 - precision_33: 0.6600 - recall_33: 0.7709\n",
      "72/72 [==============================] - 0s 3ms/step\n",
      "288/288 [==============================] - 5s 13ms/step - loss: 0.6608 - accuracy: 0.6106 - precision_34: 0.6497 - recall_34: 0.7557\n",
      "72/72 [==============================] - 0s 3ms/step\n",
      "288/288 [==============================] - 5s 13ms/step - loss: 0.6016 - accuracy: 0.6847 - precision_35: 0.7028 - recall_35: 0.8183\n",
      "72/72 [==============================] - 0s 3ms/step\n",
      "288/288 [==============================] - 5s 13ms/step - loss: 0.5971 - accuracy: 0.6903 - precision_36: 0.6982 - recall_36: 0.8485\n",
      "72/72 [==============================] - 0s 3ms/step\n",
      "288/288 [==============================] - 5s 13ms/step - loss: 0.6325 - accuracy: 0.6547 - precision_37: 0.6704 - recall_37: 0.8305\n",
      "72/72 [==============================] - 0s 3ms/step\n",
      "288/288 [==============================] - 10s 30ms/step - loss: 0.6618 - accuracy: 0.6028 - precision_38: 0.6126 - recall_38: 0.9121\n",
      "72/72 [==============================] - 0s 4ms/step\n",
      "288/288 [==============================] - 10s 30ms/step - loss: 0.6240 - accuracy: 0.6543 - precision_39: 0.6600 - recall_39: 0.8691\n",
      "72/72 [==============================] - 0s 4ms/step\n",
      "288/288 [==============================] - 10s 32ms/step - loss: 0.6548 - accuracy: 0.6142 - precision_40: 0.6158 - recall_40: 0.9423\n",
      "72/72 [==============================] - 0s 4ms/step\n",
      "288/288 [==============================] - 10s 32ms/step - loss: 0.6494 - accuracy: 0.6199 - precision_41: 0.6215 - recall_41: 0.9310\n",
      "72/72 [==============================] - 0s 4ms/step\n",
      "288/288 [==============================] - 10s 31ms/step - loss: 0.6279 - accuracy: 0.6449 - precision_42: 0.6457 - recall_42: 0.8993\n",
      "72/72 [==============================] - 0s 4ms/step\n",
      "288/288 [==============================] - 7s 20ms/step - loss: 0.6549 - accuracy: 0.6098 - precision_43: 0.6227 - recall_43: 0.8800\n",
      "72/72 [==============================] - 0s 3ms/step\n",
      "288/288 [==============================] - 7s 20ms/step - loss: 0.6695 - accuracy: 0.5974 - precision_44: 0.6150 - recall_44: 0.8722\n",
      "72/72 [==============================] - 0s 2ms/step\n",
      "288/288 [==============================] - 7s 21ms/step - loss: 0.6470 - accuracy: 0.6235 - precision_45: 0.6423 - recall_45: 0.8351\n",
      "72/72 [==============================] - 0s 2ms/step\n",
      "288/288 [==============================] - 7s 20ms/step - loss: 0.6188 - accuracy: 0.6593 - precision_46: 0.6560 - recall_46: 0.9037\n",
      "72/72 [==============================] - 0s 2ms/step\n",
      "288/288 [==============================] - 7s 20ms/step - loss: 0.6612 - accuracy: 0.6058 - precision_47: 0.6075 - recall_47: 0.9612\n",
      "72/72 [==============================] - 0s 3ms/step\n",
      "288/288 [==============================] - 7s 21ms/step - loss: 0.5313 - accuracy: 0.7275 - precision_48: 0.7423 - recall_48: 0.8330\n",
      "72/72 [==============================] - 0s 3ms/step\n",
      "288/288 [==============================] - 7s 20ms/step - loss: 0.5392 - accuracy: 0.7207 - precision_49: 0.7185 - recall_49: 0.8755\n",
      "72/72 [==============================] - 0s 3ms/step\n",
      "288/288 [==============================] - 7s 20ms/step - loss: 0.5170 - accuracy: 0.7353 - precision_50: 0.7374 - recall_50: 0.8653\n",
      "72/72 [==============================] - 0s 3ms/step\n",
      "288/288 [==============================] - 7s 20ms/step - loss: 0.5229 - accuracy: 0.7320 - precision_51: 0.7351 - recall_51: 0.8620\n",
      "72/72 [==============================] - 0s 3ms/step\n",
      "288/288 [==============================] - 7s 20ms/step - loss: 0.5387 - accuracy: 0.7244 - precision_52: 0.7175 - recall_52: 0.8886\n",
      "72/72 [==============================] - 0s 3ms/step\n",
      "288/288 [==============================] - 7s 20ms/step - loss: 0.6409 - accuracy: 0.6228 - precision_53: 0.6414 - recall_53: 0.8365\n",
      "72/72 [==============================] - 0s 5ms/step\n",
      "288/288 [==============================] - 7s 20ms/step - loss: 0.5934 - accuracy: 0.6745 - precision_54: 0.6743 - recall_54: 0.8806\n",
      "72/72 [==============================] - 0s 5ms/step\n",
      "288/288 [==============================] - 7s 20ms/step - loss: 0.6756 - accuracy: 0.5861 - precision_55: 0.6225 - recall_55: 0.7810\n",
      "72/72 [==============================] - 0s 5ms/step\n",
      "288/288 [==============================] - 7s 20ms/step - loss: 0.6177 - accuracy: 0.6525 - precision_56: 0.6632 - recall_56: 0.8502\n",
      "72/72 [==============================] - 0s 4ms/step\n",
      "288/288 [==============================] - 7s 19ms/step - loss: 0.6001 - accuracy: 0.6605 - precision_57: 0.6666 - recall_57: 0.8638\n",
      "72/72 [==============================] - 0s 4ms/step\n",
      "360/360 [==============================] - 15s 38ms/step - loss: 0.6454 - accuracy: 0.6314 - precision_58: 0.6300 - recall_58: 0.9279\n",
      "90/90 [==============================] - 1s 7ms/step\n",
      "Test accuracy: 52.70%\n",
      "Classification Error: 47.30%\n",
      "Recall: 69.20%\n",
      "Specificity: 30.15%\n",
      "Precision: 57.52%\n",
      "F1 Score: 62.82%\n",
      "AUC Score: 49.67%\n",
      "Best Hyperparameters:\n",
      "Embedding Dim: 110\n",
      "Num Units: 192\n",
      "Learning Rate: 0.0001\n"
     ]
    }
   ],
   "source": [
    "hypothesis1_test('Cleaned Text with A lemmatization', perform_hyperparameter_tuning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "ffb23ecab5824214bfb7d5aee44a597d",
  "deepnote_persisted_session": {
   "createdAt": "2023-11-03T05:01:28.941Z"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
